{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install Python packages\n!pip install numpy torch torchvision pytorch-ignite tensorboardX tensorboard opendatasets efficientnet-pytorch\n\n# Import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime as dt\n\nimport torch\nfrom torch import optim, nn\nfrom torch.utils.data import DataLoader, TensorDataset, Dataset\nfrom torchvision.utils import make_grid\nfrom torchvision import transforms as T\nfrom torchvision import datasets\n\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.metrics import Accuracy, Loss, Precision, Recall\nfrom ignite.handlers import LRScheduler, ModelCheckpoint, global_step_from_engine\nfrom ignite.contrib.handlers import ProgressBar, TensorboardLogger\nimport ignite.contrib.engines.common as common\n\nimport opendatasets as od\nimport os\nfrom random import randint\nimport urllib\nimport zipfile","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-29T18:44:31.321808Z","iopub.execute_input":"2021-11-29T18:44:31.322074Z","iopub.status.idle":"2021-11-29T18:44:39.489086Z","shell.execute_reply.started":"2021-11-29T18:44:31.322043Z","shell.execute_reply":"2021-11-29T18:44:39.487904Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"# Define device to use (CPU or GPU). CUDA = GPU support for PyTorch\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.491527Z","iopub.execute_input":"2021-11-29T18:44:39.491864Z","iopub.status.idle":"2021-11-29T18:44:39.499698Z","shell.execute_reply.started":"2021-11-29T18:44:39.491816Z","shell.execute_reply":"2021-11-29T18:44:39.498924Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"# Retrieve data directly from Stanford data source\n#!wget http://cs231n.stanford.edu/tiny-imagenet-200.zip\n  \n# Unzip raw zip file\n#!unzip -qq 'tiny-imagenet-200.zip'\n\n# Define main data directory\nDATA_DIR = 'tiny-imagenet-200' # Original images come in shapes of [3,64,64]\n\n# Define training and validation data paths\nTRAIN_DIR = os.path.join(DATA_DIR, 'train') \nVALID_DIR = os.path.join(DATA_DIR, 'val')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.501407Z","iopub.execute_input":"2021-11-29T18:44:39.501769Z","iopub.status.idle":"2021-11-29T18:44:39.508669Z","shell.execute_reply.started":"2021-11-29T18:44:39.501658Z","shell.execute_reply":"2021-11-29T18:44:39.507872Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# Functions to display single or a batch of sample images\ndef imshow(img):\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n    \ndef show_batch(dataloader):\n    dataiter = iter(dataloader)\n    images, labels = dataiter.next()    \n    imshow(make_grid(images)) # Using Torchvision.utils make_grid function\n    \ndef show_image(dataloader):\n    dataiter = iter(dataloader)\n    images, labels = dataiter.next()\n    random_num = randint(0, len(images)-1)\n    imshow(images[random_num])\n    label = labels[random_num]\n    print(f'Label: {label}, Shape: {images[random_num].shape}')","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.509852Z","iopub.execute_input":"2021-11-29T18:44:39.510595Z","iopub.status.idle":"2021-11-29T18:44:39.520488Z","shell.execute_reply.started":"2021-11-29T18:44:39.510560Z","shell.execute_reply":"2021-11-29T18:44:39.519594Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"# Setup function to create dataloaders for image datasets\ndef generate_dataloader(data, name, transform):\n    if data is None: \n        return None\n    \n    # Read image files to pytorch dataset using ImageFolder, a generic data \n    # loader where images are in format root/label/filename\n    # See https://pytorch.org/vision/stable/datasets.html\n    if transform is None:\n        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n    else:\n        dataset = datasets.ImageFolder(data, transform=transform)\n\n    # Set options for device\n    if use_cuda:\n        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n    else:\n        kwargs = {}\n    \n    # Wrap image dataset (defined above) in dataloader \n    dataloader = DataLoader(dataset, batch_size=batch_size, \n                        shuffle=(name==\"train\"), \n                        **kwargs)\n    \n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.523442Z","iopub.execute_input":"2021-11-29T18:44:39.524215Z","iopub.status.idle":"2021-11-29T18:44:39.535001Z","shell.execute_reply.started":"2021-11-29T18:44:39.524176Z","shell.execute_reply":"2021-11-29T18:44:39.534135Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"# Create separate validation subfolders for the validation images based on\n# their labels indicated in the val_annotations txt file\nval_img_dir = os.path.join(VALID_DIR, 'images')\n\n# Open and read val annotations text file\nfp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\ndata = fp.readlines()\n\n# Create dictionary to store img filename (word 0) and corresponding\n# label (word 1) for every line in the txt file (as key value pair)\nval_img_dict = {}\nfor line in data:\n    words = line.split('\\t')\n    val_img_dict[words[0]] = words[1]\nfp.close()\n\n# Display first 10 entries of resulting val_img_dict dictionary\n{k: val_img_dict[k] for k in list(val_img_dict)[:10]}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.536348Z","iopub.execute_input":"2021-11-29T18:44:39.537108Z","iopub.status.idle":"2021-11-29T18:44:39.559162Z","shell.execute_reply.started":"2021-11-29T18:44:39.537070Z","shell.execute_reply":"2021-11-29T18:44:39.558396Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"# Create subfolders (if not present) for validation images based on label,\n# and move images into the respective folders\nfor img, folder in val_img_dict.items():\n    newpath = (os.path.join(val_img_dir, folder))\n    if not os.path.exists(newpath):\n        os.makedirs(newpath)\n    if os.path.exists(os.path.join(val_img_dir, img)):\n        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.560497Z","iopub.execute_input":"2021-11-29T18:44:39.561315Z","iopub.status.idle":"2021-11-29T18:44:39.668242Z","shell.execute_reply.started":"2021-11-29T18:44:39.561273Z","shell.execute_reply":"2021-11-29T18:44:39.667621Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Define transformation sequence for image pre-processing\n# If not using pre-trained model, normalize with 0.5, 0.5, 0.5 (mean and SD)\n# If using pre-trained ImageNet, normalize with mean=[0.485, 0.456, 0.406], \n# std=[0.229, 0.224, 0.225])\n\npreprocess_transform_pretrain = T.Compose([\n                T.Resize(256), # Resize images to 256 x 256\n                T.CenterCrop(224), # Center crop image\n                T.RandomHorizontalFlip(),\n                T.ToTensor(),  # Converting cropped images to tensors\n                T.Normalize(mean=[0.485, 0.456, 0.406], \n                            std=[0.229, 0.224, 0.225])\n])","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.670310Z","iopub.execute_input":"2021-11-29T18:44:39.670826Z","iopub.status.idle":"2021-11-29T18:44:39.676900Z","shell.execute_reply.started":"2021-11-29T18:44:39.670784Z","shell.execute_reply":"2021-11-29T18:44:39.676236Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"# Define batch size for DataLoaders\nbatch_size = 15\n\n# Create DataLoaders for pre-trained models (normalized based on specific requirements)\n\n\n\ntrain_loader_pretrain = generate_dataloader(TRAIN_DIR, \"train\",\n                                  transform=preprocess_transform_pretrain)\n\nval_loader_pretrain = generate_dataloader(val_img_dir, \"val\",\n                                 transform=preprocess_transform_pretrain)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:39.679848Z","iopub.execute_input":"2021-11-29T18:44:39.680038Z","iopub.status.idle":"2021-11-29T18:44:40.332376Z","shell.execute_reply.started":"2021-11-29T18:44:39.680009Z","shell.execute_reply":"2021-11-29T18:44:40.331551Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Define model architecture (using efficientnet-b3 version)\nfrom efficientnet_pytorch import EfficientNet\nmodel = EfficientNet.from_pretrained('efficientnet-b3', num_classes=200)\n\n# Move model to designated device (Use GPU when on Colab)\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:40.334993Z","iopub.execute_input":"2021-11-29T18:44:40.335539Z","iopub.status.idle":"2021-11-29T18:44:42.140014Z","shell.execute_reply.started":"2021-11-29T18:44:40.335496Z","shell.execute_reply":"2021-11-29T18:44:42.139090Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# Define hyperparameters and settings\nlr = 0.001  # Learning rate\nnum_epochs = 6  # Number of epochs\nlog_interval = 300  # Number of iterations before logging\n\n# Set loss function (categorical Cross Entropy Loss)\nloss_func = nn.CrossEntropyLoss()\n\n# Set optimizer (using Adam as default)\noptimizer = optim.Adam(model.parameters(), lr=lr)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:42.141483Z","iopub.execute_input":"2021-11-29T18:44:42.141797Z","iopub.status.idle":"2021-11-29T18:44:42.152002Z","shell.execute_reply.started":"2021-11-29T18:44:42.141740Z","shell.execute_reply":"2021-11-29T18:44:42.151141Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"# Setup pytorch-ignite trainer engine\ntrainer = create_supervised_trainer(model, optimizer, loss_func, device=device)\n\n# Add progress bar to monitor model training\nProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"Batch Loss\": x})","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:42.153714Z","iopub.execute_input":"2021-11-29T18:44:42.154203Z","iopub.status.idle":"2021-11-29T18:44:42.163193Z","shell.execute_reply.started":"2021-11-29T18:44:42.154158Z","shell.execute_reply":"2021-11-29T18:44:42.162441Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"# Define evaluation metrics\nmetrics = {\n    \"accuracy\": Accuracy(), \n    \"loss\": Loss(loss_func),\n}","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:42.164417Z","iopub.execute_input":"2021-11-29T18:44:42.165221Z","iopub.status.idle":"2021-11-29T18:44:42.175166Z","shell.execute_reply.started":"2021-11-29T18:44:42.165184Z","shell.execute_reply":"2021-11-29T18:44:42.174396Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Evaluator for training data\ntrain_evaluator = create_supervised_evaluator(model, metrics=metrics, device=device)\n\n# Evaluator for validation data\nevaluator = create_supervised_evaluator(model, metrics=metrics, device=device)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:42.178832Z","iopub.execute_input":"2021-11-29T18:44:42.179198Z","iopub.status.idle":"2021-11-29T18:44:42.187821Z","shell.execute_reply.started":"2021-11-29T18:44:42.179170Z","shell.execute_reply":"2021-11-29T18:44:42.186880Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Display message to indicate start of training\n@trainer.on(Events.STARTED)\ndef start_message():\n    print(\"Begin training\")\n\n# Log results from every batch\n@trainer.on(Events.ITERATION_COMPLETED(every=log_interval))\ndef log_batch(trainer):\n    batch = (trainer.state.iteration - 1) % trainer.state.epoch_length + 1\n    print(f\"Epoch {trainer.state.epoch} / {num_epochs}, \"\n          f\"Batch {batch} / {trainer.state.epoch_length}: \"\n          f\"Loss: {trainer.state.output:.3f}\")\n\n# Evaluate and print training set metrics\n@trainer.on(Events.EPOCH_COMPLETED)\ndef log_training_loss(trainer):\n    print(f\"Epoch [{trainer.state.epoch}] - Loss: {trainer.state.output:.2f}\")\n    train_evaluator.run(train_loader_pretrain)\n    epoch = trainer.state.epoch\n    metrics = train_evaluator.state.metrics\n    print(f\"Train - Loss: {metrics['loss']:.3f}, \"\n          f\"Accuracy: {metrics['accuracy']:.3f} \")\n\n# Evaluate and print validation set metrics\n@trainer.on(Events.EPOCH_COMPLETED)\ndef log_validation_loss(trainer):\n    evaluator.run(val_loader_pretrain)\n    epoch = trainer.state.epoch\n    metrics = evaluator.state.metrics\n    print(f\"Validation - Loss: {metrics['loss']:.3f}, \"\n          f\"Accuracy: {metrics['accuracy']:.3f}\")\n\n# Sets up checkpoint handler to save best n model(s) based on validation accuracy metric\ncommon.save_best_model_by_val_score(\n          output_path=\"best_models\",\n          evaluator=evaluator, model=model,\n          metric_name=\"accuracy\", n_saved=1,\n          trainer=trainer, tag=\"val\")","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:42.190710Z","iopub.execute_input":"2021-11-29T18:44:42.191281Z","iopub.status.idle":"2021-11-29T18:44:42.207378Z","shell.execute_reply.started":"2021-11-29T18:44:42.191234Z","shell.execute_reply":"2021-11-29T18:44:42.206428Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"# Define a Tensorboard logger\ntb_logger = TensorboardLogger(log_dir=\"logs\")\n\n# Attach handler to plot trainer's loss every n iterations\ntb_logger.attach_output_handler(\n    trainer,\n    event_name=Events.ITERATION_COMPLETED(every=log_interval),\n    tag=\"training\",\n    output_transform=lambda loss: {\"Batch Loss\": loss}\n)\n\n# Attach handler to dump evaluator's metrics every epoch completed\nfor tag, evaluator in [(\"training\", train_evaluator), (\"validation\", evaluator)]:\n    tb_logger.attach_output_handler(\n        evaluator,\n        event_name=Events.EPOCH_COMPLETED,\n        tag=tag,\n        metric_names=\"all\",\n        global_step_transform=global_step_from_engine(trainer),\n    )","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:42.209068Z","iopub.execute_input":"2021-11-29T18:44:42.209360Z","iopub.status.idle":"2021-11-29T18:44:42.223918Z","shell.execute_reply.started":"2021-11-29T18:44:42.209317Z","shell.execute_reply":"2021-11-29T18:44:42.223071Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# Start training\ntrainer.run(train_loader_pretrain, max_epochs=num_epochs)","metadata":{"execution":{"iopub.status.busy":"2021-11-29T18:44:42.226646Z","iopub.execute_input":"2021-11-29T18:44:42.227230Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(evaluator.state.metrics)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tar -zcf logs.tar.gz logs2","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}

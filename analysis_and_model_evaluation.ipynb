{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "COMPARISON_EXAMPLE.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc8vy5b_cDzZ"
      },
      "source": [
        "# Highest score in Tiny ImageNet validation dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nvilBOIcDzd"
      },
      "source": [
        "The goal was to achieve the highest possible (at least 55%) accuracy in the Tiny ImageNet validation dataset.\n",
        "The sample code provided, which utilizes ResNet18 (not pre-trained?) model achieved 50% accuracy after 30 epochs.\n",
        "\n",
        "First I tried training with sample code and different combinations of augmentations. Not all of the tries successfully trained for 30 epochs, but those that did (original default.yaml for example) had a worse or negligible difference in performance. \n",
        "\n",
        "Then I tried modifying the number of epochs.\n",
        "The example code achieved an accuracy of about 55% at aroud 50 epochs, at which point the accuracy plateaued.\n",
        "\n",
        "I tried to use a different network with the sample code provided but the files I needed to change were all over the place. So I decided to follow a tutorial in [TDS article](https://towardsdatascience.com/pytorch-ignite-classifying-tiny-imagenet-with-efficientnet-e5b1768e5e8f). This example used EfficientNet-B3 pretrained network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN7MR7eScDzi",
        "outputId": "517d9e6d-c460-49c5-e815-2652ad51f271"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "\n",
        "os.chdir('..')\n",
        "os.getcwd()\n",
        "\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
        "from functools import reduce\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "print(f'Use {device}')\n",
        "\n",
        "from importlib import reload \n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Use cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJbcMKvDcDzk"
      },
      "source": [
        "### Visualizing tensorboard stats"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Yxa629XcDzl"
      },
      "source": [
        "def read_tensorboard(path, tags=['training/accuracy', 'validation/accuracy']):\n",
        "    ea =  EventAccumulator(path)\n",
        "    ea.Reload()\n",
        "    dfs = []\n",
        "    for tag in tags:\n",
        "        df = pd.DataFrame(ea.Scalars(tag)).drop('wall_time', 1)\n",
        "        df.columns = ['step', tag]\n",
        "        dfs.append(df)\n",
        "    dfs = reduce(lambda left, right: pd.merge(left, right, on='step'), dfs)\n",
        "    return dfs"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "0-4Bu8ogcDzl",
        "outputId": "55c9fdb4-c451-42eb-dcd5-b7a807289c29"
      },
      "source": [
        "training_v1 = read_tensorboard('content/events.out.tfevents.1638201141.cbdab2f461ad')\n",
        "\n",
        "training = pd.concat([\n",
        "    training_v1.assign(model='efficientnet_test0'),\n",
        "])\n",
        "training = training.melt(id_vars=['step', 'model'], value_name='accuracy', var_name='split')\n",
        "\n",
        "training.split = training.split.str.replace('_acc', '')\n",
        "\n",
        "training.head(10)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>step</th>\n",
              "      <th>model</th>\n",
              "      <th>split</th>\n",
              "      <th>accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>efficientnet_test0</td>\n",
              "      <td>training/accuracy</td>\n",
              "      <td>0.52830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>efficientnet_test0</td>\n",
              "      <td>training/accuracy</td>\n",
              "      <td>0.62055</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>efficientnet_test0</td>\n",
              "      <td>training/accuracy</td>\n",
              "      <td>0.66301</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>efficientnet_test0</td>\n",
              "      <td>validation/accuracy</td>\n",
              "      <td>0.48870</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "      <td>efficientnet_test0</td>\n",
              "      <td>validation/accuracy</td>\n",
              "      <td>0.55140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>3</td>\n",
              "      <td>efficientnet_test0</td>\n",
              "      <td>validation/accuracy</td>\n",
              "      <td>0.56940</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   step               model                split  accuracy\n",
              "0     1  efficientnet_test0    training/accuracy   0.52830\n",
              "1     2  efficientnet_test0    training/accuracy   0.62055\n",
              "2     3  efficientnet_test0    training/accuracy   0.66301\n",
              "3     1  efficientnet_test0  validation/accuracy   0.48870\n",
              "4     2  efficientnet_test0  validation/accuracy   0.55140\n",
              "5     3  efficientnet_test0  validation/accuracy   0.56940"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rmYYmZFPcDzl"
      },
      "source": [
        "Plot training curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "2ICkvqDycDzm",
        "outputId": "2254c340-9343-425a-888c-3ac8101ebdd3"
      },
      "source": [
        "sns.lineplot(x='step', y='accuracy', hue='model', style='split', data=training)\n",
        "plt.show()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXq04nAccDzm"
      },
      "source": [
        "After training for 3 epochs the model achieved a validation accuracy of 57%.\n",
        "Since the training ran for 3 epochs only (1h in Kaggle/2h in Colab), it is hard to say what is the limit of this model if given more computation time. Probaby could reach 60% by 10 epochs?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdRwsX7TGP13"
      },
      "source": [
        "## Evaluate trained weights\n",
        "Upload weights file \"default_efficientnet_accyracy_0.5694.pt\" to colab directory 'content/'.\\\n",
        "Run code blocks below to evaluate trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rwRX4-bfrG0"
      },
      "source": [
        "##### Use our trained weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6AUSkrELUJM",
        "outputId": "e9621a24-9d99-40b4-df5b-bf8b1e0f98d3"
      },
      "source": [
        "!pip install efficientnet-pytorch\n",
        "\n",
        "# Define model architecture (using efficientnet-b3 version)\n",
        "from efficientnet_pytorch import EfficientNet\n",
        "model = EfficientNet.from_pretrained('efficientnet-b3', num_classes=200)\n",
        "\n",
        "state_dict = torch.load('content/default_efficientnet_accuracy_0.5694.pt', map_location=torch.device('cpu'))\n",
        "model.load_state_dict(state_dict)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: efficientnet-pytorch in /usr/local/lib/python3.7/dist-packages (0.7.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch) (3.10.0.2)\n",
            "Loaded pretrained weights for efficientnet-b3\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvZ5m8vwf4YD"
      },
      "source": [
        "#### Create validation dataloader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34tjNCwDGPXs"
      },
      "source": [
        "from torchvision import transforms as T\n",
        "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
        "\n",
        "\n",
        "# Define device to use (CPU or GPU). CUDA = GPU support for PyTorch\n",
        "use_cuda = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "# Setup function to create dataloaders for image datasets\n",
        "def generate_dataloader(data, name, transform):\n",
        "    if data is None: \n",
        "        return None\n",
        "    \n",
        "    # Read image files to pytorch dataset using ImageFolder, a generic data \n",
        "    # loader where images are in format root/label/filename\n",
        "    # See https://pytorch.org/vision/stable/datasets.html\n",
        "    if transform is None:\n",
        "        dataset = datasets.ImageFolder(data, transform=T.ToTensor())\n",
        "    else:\n",
        "        dataset = datasets.ImageFolder(data, transform=transform)\n",
        "\n",
        "    # Set options for device\n",
        "    if use_cuda:\n",
        "        kwargs = {\"pin_memory\": True, \"num_workers\": 1}\n",
        "    else:\n",
        "        kwargs = {}\n",
        "    \n",
        "    # Wrap image dataset (defined above) in dataloader \n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, \n",
        "                        shuffle=(name==\"train\"), \n",
        "                        **kwargs)\n",
        "    \n",
        "    return dataloader\n",
        "\n",
        "# Create separate validation subfolders for the validation images based on\n",
        "# their labels indicated in the val_annotations txt file\n",
        "val_img_dir = os.path.join(VALID_DIR, 'images')\n",
        "\n",
        "# Open and read val annotations text file\n",
        "fp = open(os.path.join(VALID_DIR, 'val_annotations.txt'), 'r')\n",
        "data = fp.readlines()\n",
        "\n",
        "# Create dictionary to store img filename (word 0) and corresponding\n",
        "# label (word 1) for every line in the txt file (as key value pair)\n",
        "val_img_dict = {}\n",
        "for line in data:\n",
        "    words = line.split('\\t')\n",
        "    val_img_dict[words[0]] = words[1]\n",
        "fp.close()\n",
        "\n",
        "# Display first 10 entries of resulting val_img_dict dictionary\n",
        "{k: val_img_dict[k] for k in list(val_img_dict)[:10]}\n",
        "\n",
        "\n",
        "# Create subfolders (if not present) for validation images based on label,\n",
        "# and move images into the respective folders\n",
        "for img, folder in val_img_dict.items():\n",
        "    newpath = (os.path.join(val_img_dir, folder))\n",
        "    if not os.path.exists(newpath):\n",
        "        os.makedirs(newpath)\n",
        "    if os.path.exists(os.path.join(val_img_dir, img)):\n",
        "        os.rename(os.path.join(val_img_dir, img), os.path.join(newpath, img))\n",
        "\n",
        "# Create DataLoader\n",
        "val_loader_pretrain = generate_dataloader(val_img_dir, \"val\",\n",
        "                                 transform=None)"
      ],
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6_gX0jRf-Iq"
      },
      "source": [
        "#### Measure model accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "29lwqxVPdkaz",
        "outputId": "e3412324-8933-4d52-e1c9-49fb396d5252"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "for data in val_loader_pretrain:\n",
        "    images, labels = data\n",
        "    outputs = model(Variable(images))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    print(\"pred: \", predicted)\n",
        "    print(\"Act: \", labels)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum()\n",
        "    #print(f'Progress --- total: {total}, correct: {correct}')\n",
        "print('Accuracy of the network on the 10000 test images: ', (100 * correct / total))"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred:  tensor([ 55,  69, 144,   0,  85, 137, 152, 119,  44,  67, 196,   5, 146,  67,\n",
            "         55])\n",
            "Act:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "pred:  tensor([152,  62, 164, 174, 118,   4,  69, 147,  83,  88, 192,  37,  80, 167,\n",
            "         69])\n",
            "Act:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "pred:  tensor([171, 140, 189,  96,  44,  85,  69,  40,  88,  71, 142, 182,  53, 135,\n",
            "        188])\n",
            "Act:  tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
            "pred:  tensor([ 88,  36, 175, 148,   0,  48,  40, 146,  61,  39,  67, 127, 196,  85,\n",
            "         75])\n",
            "Act:  tensor([0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "pred:  tensor([125,  73, 154,  82, 146, 178,  91,  75, 137,   9, 182,  96,  23,  40,\n",
            "        114])\n",
            "Act:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "pred:  tensor([154, 147, 189, 144, 145, 146,  82, 151,  85, 171,  60, 145, 186,  72,\n",
            "         40])\n",
            "Act:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n",
            "pred:  tensor([162, 196, 142,  40, 174,  36,  96, 146,   1,   9,  62,  33,  80,   5,\n",
            "        194])\n",
            "Act:  tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2])\n",
            "pred:  tensor([ 21, 180,  38,  62,  85, 164, 195, 118,  72, 184,  53, 145,  87,  19,\n",
            "        117])\n",
            "Act:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
            "pred:  tensor([131, 103, 194,  17,  87,  88,  35, 135, 134,  42, 147,  59, 196,  69,\n",
            "        181])\n",
            "Act:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
            "pred:  tensor([ 87, 184,  19, 123, 188, 119,  51, 157, 103, 140, 140,  18, 144,  50,\n",
            "        126])\n",
            "Act:  tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
            "pred:  tensor([ 29, 188,  87,  38, 126, 145,  69, 169,  69,  96,  21,  93, 107,  39,\n",
            "        146])\n",
            "Act:  tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
            "pred:  tensor([147,  64, 108, 173,  39,  22, 169, 184, 125,  83,  61, 181,  10, 156,\n",
            "        114])\n",
            "Act:  tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
            "pred:  tensor([ 55,  74,  81,  31,  85, 122,  96, 144,  93, 199, 196, 147, 198, 146,\n",
            "         98])\n",
            "Act:  tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])\n",
            "pred:  tensor([118,   0,  32, 144,  29, 188, 108,  67, 164,  84, 199,  85,  38,  34,\n",
            "        137])\n",
            "Act:  tensor([3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
            "pred:  tensor([175,  75,  88,  67,  89,  71,  69,  94,   5, 154, 145, 186,  31,  39,\n",
            "         29])\n",
            "Act:  tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
            "pred:  tensor([ 10, 146,  52, 159, 196, 103, 112,  69,  26,  60, 157,  80, 197, 194,\n",
            "        187])\n",
            "Act:  tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4])\n",
            "pred:  tensor([ 21, 170, 136, 188,  91, 171, 188,  46,  82,  72,  46,  39, 108,  88,\n",
            "         68])\n",
            "Act:  tensor([4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5])\n",
            "pred:  tensor([162,  91,   2,  55,  98, 189,  62, 142, 169,  32,  87,  88, 171, 145,\n",
            "        151])\n",
            "Act:  tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "pred:  tensor([ 81,   4,  88, 131, 145, 118, 176, 185,  93, 117, 188,  92,  69, 174,\n",
            "         85])\n",
            "Act:  tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "pred:  tensor([148, 183, 175, 126,  34,  89,  91, 195, 137, 147,  59, 193,  67, 163,\n",
            "        196])\n",
            "Act:  tensor([5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5])\n",
            "pred:  tensor([196, 114, 106,  37, 137,  13, 149, 183, 175,  33, 169, 143, 153, 108,\n",
            "        126])\n",
            "Act:  tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n",
            "pred:  tensor([147, 129, 188, 169, 195, 144, 196,  91,  55, 111, 196,  88, 119, 186,\n",
            "         87])\n",
            "Act:  tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n",
            "pred:  tensor([107, 179, 196,  67, 110, 146,  60, 174,  14, 151, 177,  54,  84, 170,\n",
            "        198])\n",
            "Act:  tensor([6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6])\n",
            "pred:  tensor([182, 169, 135,  60, 127, 116,  17,  95,  40,  63,  18, 104,  80, 162,\n",
            "        102])\n",
            "Act:  tensor([6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
            "pred:  tensor([  4, 198, 103,  82,  13, 192,  44, 171, 117, 189, 185, 175, 145,  28,\n",
            "         36])\n",
            "Act:  tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
            "pred:  tensor([196, 135,  83, 136, 151, 115, 141,  67, 184,  29,  85,  42,  15,  65,\n",
            "         54])\n",
            "Act:  tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7])\n",
            "pred:  tensor([ 39,   6, 144, 154, 164,  78,  86,  81,   6,  67, 167,   0, 132, 188,\n",
            "        100])\n",
            "Act:  tensor([7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8])\n",
            "pred:  tensor([162,  91, 181, 149,  87, 196,  35, 125,  84, 151, 129,  94,  26, 174,\n",
            "         93])\n",
            "Act:  tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "pred:  tensor([107,   2,  17,  52, 121,  62, 184,  39,  97,  16, 140, 141,  57, 107,\n",
            "        144])\n",
            "Act:  tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "pred:  tensor([ 90, 143, 145,  33, 119,   0,  76, 157, 194, 124,  98,  37,  92, 111,\n",
            "          6])\n",
            "Act:  tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8])\n",
            "pred:  tensor([134, 170,  10,  76, 175,  58,  99,   9,  23, 104,  45,  11, 185,  69,\n",
            "          2])\n",
            "Act:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n",
            "pred:  tensor([ 54,  23,  62, 146,   6,  63, 191, 129, 169, 190, 119, 188,  66,  26,\n",
            "         55])\n",
            "Act:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n",
            "pred:  tensor([107, 139,  46, 177,  40, 180,  90,  39, 195, 126,  21, 145,  38, 119,\n",
            "         13])\n",
            "Act:  tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9])\n",
            "pred:  tensor([ 98, 190,  18, 189,  55, 109, 139, 150, 110,  38, 136,   0,  69, 182,\n",
            "         22])\n",
            "Act:  tensor([ 9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
            "pred:  tensor([ 60, 168,  67, 136,  93,   9, 144, 188, 198,  62, 117,  69, 181, 122,\n",
            "         36])\n",
            "Act:  tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
            "pred:  tensor([  4,  44, 141,  69, 196,  67, 171,  66,  59,  74,  99,  61,   9, 130,\n",
            "        172])\n",
            "Act:  tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])\n",
            "pred:  tensor([ 39,  19,   0, 100, 169,  87, 140, 144,  35, 126,  51, 171, 166, 146,\n",
            "        103])\n",
            "Act:  tensor([10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11])\n",
            "pred:  tensor([ 40, 135, 136,  84, 141,  75, 194, 137, 196,  88, 185,  60,  90, 189,\n",
            "          9])\n",
            "Act:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
            "pred:  tensor([193,  40, 154, 198,   1,  77, 168,  97,  50,  93,  63, 184, 196,  49,\n",
            "         92])\n",
            "Act:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
            "pred:  tensor([145,  89,  98, 184, 196,  91, 177,  29,  87,  14, 100,  55, 194,  78,\n",
            "         74])\n",
            "Act:  tensor([11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11, 11])\n",
            "pred:  tensor([ 61, 184,  76,  64,   7,  55, 136, 189,  57,  71,  89, 148, 193, 140,\n",
            "        194])\n",
            "Act:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
            "pred:  tensor([  2,  18,  34,  65, 103,  75, 194,  81, 108,  35, 142,  14,   0,  95,\n",
            "         90])\n",
            "Act:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
            "pred:  tensor([141, 142,  75, 183,  80, 154,  68, 162, 169,  69, 192, 115, 140, 194,\n",
            "         32])\n",
            "Act:  tensor([12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12])\n",
            "pred:  tensor([170, 154, 195, 136,  78,  80, 144, 108,  68, 182, 145,  13, 145,  75,\n",
            "        196])\n",
            "Act:  tensor([12, 12, 12, 12, 12, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13])\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-93-4fc0ccefda40>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_loader_pretrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"pred: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \"\"\"\n\u001b[1;32m    313\u001b[0m         \u001b[0;31m# Convolution layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m         \u001b[0;31m# Pooling and final linear layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_avg_pooling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mdrop_connect_rate\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_blocks\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# scale drop connect_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 296\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdrop_connect_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    297\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0;31m# Head\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, drop_connect_rate)\u001b[0m\n\u001b[1;32m    105\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_expand_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bn0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_swish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_depthwise_conv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mMemoryEfficientSwish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mSwishImplementation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/efficientnet_pytorch/utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(ctx, i)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_for_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB0KhFXhgB53"
      },
      "source": [
        "Can't get validation dataloader set up correctly, labels are not correct. The trained weights should still be 57% accurate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eopr2DGdBxvV"
      },
      "source": [
        "Other notes:\\\n",
        "90% of trainings crashed when using provided sample code which heavily limited number of tests with different parameters.\n",
        "\n",
        "After using TowardsDataScience example no major problems occured. Sometimes only GPU out of memory error.\n",
        "\n",
        "\n"
      ]
    }
  ]
}